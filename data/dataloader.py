import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import h5py
from pathlib import Path
from typing import Tuple, Optional, List
import warnings
from collections import Counter

class MATESimDataset(Dataset):
    """
    Real HDF5 dataset loader for MATE simulation data.
    
    Loads data from HDF5 files generated by MATESIM toolkit for particle classification
    in the MATE-TPC experiment.
    
    Data Format (Y-Z Projection):
        - images: (N, 80, 48, 2) - Y-Z plane projection
            - Channel 0: Charge deposition (energy)
            - Channel 1: Drift time proxy (X-coordinate)
        - physics_features: (N, 4) - Moment of Inertia tensor features (I_xx, I_yy, I_xy, Eigen_Ratio)
        - labels: (N,) - Particle type labels
    
    Detector Dimensions:
        - Y: [-150, 150] mm (mapped to 80 pixels)
        - Z: [0, 300] mm (mapped to 48 pixels)
        - X (drift): [-100, 100] mm (encoded in Channel 1)
    
    Note: For legacy 64x64 data, set img_height=64, img_width=64.
    """
    def __init__(self, 
                 data_dir: str = "../dataset/HDF5_Form",
                 mode: str = 'train',
                 train_split: float = 0.8,
                 max_samples_per_class: Optional[int] = None,
                 normalize: bool = True,
                 seed: int = 42,
                 file_mapping: Optional[dict] = None,
                 img_height: int = 80,
                 img_width: int = 48):
        """
        Initialize MATE simulation dataset.
        
        Args:
            data_dir: Directory containing HDF5 files
            mode: 'train', 'val', or 'test'
            train_split: Fraction of data to use for training (rest split equally for val/test)
            max_samples_per_class: Maximum samples to load per class (None = load all)
            normalize: Whether to normalize images to [0, 1]
            seed: Random seed for reproducible train/val/test split
            file_mapping: Custom file mapping {label: filename}. If None, uses default binary mapping.
            img_height: Expected image height (default: 80 for Y-Z projection)
            img_width: Expected image width (default: 48 for Y-Z projection)
        """
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.normalize = normalize
        self.img_height = img_height
        self.img_width = img_width
        
        # Define file paths for binary classification: 3He (label 0) vs 4He (label 1)
        if file_mapping is None:
            self.file_mapping = {
                0: "sim_inv_12C300MeV_4He_3He_25k.h5",  # 3He particles
                1: "sim_inv_12C300MeV_4He_4He_25k.h5"   # 4He (alpha) particles
            }
        else:
            self.file_mapping = file_mapping
        
        # Load data from HDF5 files
        self.images, self.physics_features, self.labels = self._load_data(max_samples_per_class)
        
        # Split data into train/val/test
        self._split_data(train_split, seed)
        
        print(f"  Loaded {len(self)} samples for {mode} mode")
        
    def _load_data(self, max_samples: Optional[int]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Load data from HDF5 files."""
        all_images = []
        all_physics = []
        all_labels = []
        
        for label, filename in self.file_mapping.items():
            filepath = self.data_dir / filename
            
            if not filepath.exists():
                raise FileNotFoundError(
                    f"Data file not found: {filepath}\n"
                    f"Please ensure MATE simulation data is available in {self.data_dir}"
                )
            
            with h5py.File(filepath, 'r') as f:
                # Load images
                images = f['images'][:]
                
                # Validate image shape
                expected_shape = (images.shape[0], self.img_height, self.img_width, 2)
                if images.shape[1:] != (self.img_height, self.img_width, 2):
                    # Check for legacy 64x64 format
                    if images.shape[1:] == (64, 64, 2):
                        warnings.warn(
                            f"Detected legacy 64x64 image format in {filename}. "
                            f"Consider re-converting data to 80x48 Y-Z projection format."
                        )
                        self.img_height = 64
                        self.img_width = 64
                    else:
                        raise ValueError(
                            f"Unexpected image shape in {filename}: {images.shape}. "
                            f"Expected (N, {self.img_height}, {self.img_width}, 2)"
                        )
                
                # Load physics features (N, 4)
                physics = f['physics_features'][:]
                
                # Limit samples if specified
                if max_samples is not None:
                    images = images[:max_samples]
                    physics = physics[:max_samples]
                
                n_samples = len(images)
                
                all_images.append(images)
                all_physics.append(physics)
                all_labels.append(np.full(n_samples, label, dtype=np.int64))
                
                print(f"  Loaded {n_samples} samples from {filename} (label={label})")
        
        # Concatenate all data
        images = np.concatenate(all_images, axis=0)
        physics = np.concatenate(all_physics, axis=0)
        labels = np.concatenate(all_labels, axis=0)
        
        return images, physics, labels
    
    def _split_data(self, train_split: float, seed: int):
        """Split data into train/val/test sets (stratified per class when possible)."""
        rng = np.random.default_rng(seed)
        labels = self.labels
        all_indices = np.arange(len(labels))

        # Build stratified splits by class label. This avoids class imbalance drift
        # in each split and matches typical scientific reporting expectations.
        unique_labels, counts = np.unique(labels, return_counts=True)

        # If any class has too few samples, we can still split, but val/test may be empty.
        # We keep behavior robust and print a clear warning for users.
        if np.any(counts < 2):
            warnings.warn(
                "Some classes have <2 samples; stratified splitting may produce empty val/test "
                "for those classes. Consider providing more data per class."
            )

        train_idx: list[int] = []
        val_idx: list[int] = []
        test_idx: list[int] = []

        for c in unique_labels.tolist():
            c_idx = all_indices[labels == c]
            rng.shuffle(c_idx)

            n_c = len(c_idx)
            n_train_c = int(n_c * train_split)
            n_remaining = n_c - n_train_c
            n_val_c = int(n_remaining / 2)
            n_test_c = n_remaining - n_val_c

            train_idx.extend(c_idx[:n_train_c].tolist())
            val_idx.extend(c_idx[n_train_c:n_train_c + n_val_c].tolist())
            test_idx.extend(c_idx[n_train_c + n_val_c:n_train_c + n_val_c + n_test_c].tolist())

        # Shuffle within each split to avoid class-order artifacts
        rng.shuffle(train_idx)
        rng.shuffle(val_idx)
        rng.shuffle(test_idx)

        # Print split statistics (high-signal for debugging)
        train_counts = Counter(labels[np.array(train_idx, dtype=np.int64)].tolist()) if train_idx else {}
        val_counts = Counter(labels[np.array(val_idx, dtype=np.int64)].tolist()) if val_idx else {}
        test_counts = Counter(labels[np.array(test_idx, dtype=np.int64)].tolist()) if test_idx else {}
        print(f"  Split counts (by label):")
        print(f"    train: {dict(sorted(train_counts.items()))}")
        print(f"    val:   {dict(sorted(val_counts.items()))}")
        print(f"    test:  {dict(sorted(test_counts.items()))}")

        if self.mode == 'train':
            indices = np.array(train_idx, dtype=np.int64)
        elif self.mode == 'val':
            indices = np.array(val_idx, dtype=np.int64)
        elif self.mode == 'test':
            indices = np.array(test_idx, dtype=np.int64)
        else:
            raise ValueError(f"Invalid mode: {self.mode}. Must be 'train', 'val', or 'test'")

        self.images = self.images[indices]
        self.physics_features = self.physics_features[indices]
        self.labels = self.labels[indices]
    
    def __len__(self) -> int:
        return len(self.labels)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get a single sample.
        
        Returns:
            Tuple of (image, physics_features, label)
                - image: (2, H, W) tensor (default: (2, 80, 48) for Y-Z projection)
                - physics_features: (4,) tensor
                - label: scalar tensor
        """
        # Load image (H, W, 2) and convert to (2, H, W)
        image = self.images[idx].transpose(2, 0, 1).astype(np.float32)
        
        # Normalize to [0, 1] if requested
        if self.normalize:
            # Normalize each channel independently
            for c in range(image.shape[0]):
                channel = image[c]
                min_val, max_val = channel.min(), channel.max()
                if max_val > min_val:
                    image[c] = (channel - min_val) / (max_val - min_val)
        
        # Load physics features
        physics = self.physics_features[idx].astype(np.float32)
        
        # Load label
        label = self.labels[idx]
        
        return torch.from_numpy(image), torch.from_numpy(physics), torch.tensor(label, dtype=torch.long)


def get_dataloader(batch_size: int = 32, 
                   mode: str = 'train',
                   data_dir: str = "../dataset/HDF5_Form",
                   num_workers: int = 0,
                   max_samples_per_class: Optional[int] = None,
                   file_mapping: Optional[dict] = None,
                   img_height: int = 80,
                   img_width: int = 48) -> DataLoader:
    """
    Factory function for MATE DataLoaders.
    
    Args:
        batch_size: Batch size for training
        mode: 'train', 'val', or 'test'
        data_dir: Directory containing HDF5 files
        num_workers: Number of worker processes for data loading
                    (Set to 0 on Windows to avoid multiprocessing issues)
        max_samples_per_class: Maximum samples per class (for quick testing)
        file_mapping: Custom file mapping {label: filename}
        img_height: Expected image height (default: 80)
        img_width: Expected image width (default: 48)
    
    Returns:
        DataLoader instance
    """
    dataset = MATESimDataset(
        data_dir=data_dir,
        mode=mode,
        max_samples_per_class=max_samples_per_class,
        file_mapping=file_mapping,
        img_height=img_height,
        img_width=img_width
    )
    
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=(mode == 'train'),
        num_workers=num_workers,
        pin_memory=True
    )


if __name__ == "__main__":
    # Test the dataloader
    print("=== Testing MATE DataLoader ===")
    
    try:
        # Test with small sample for quick verification
        print("\n--- Attempting to load data ---")
        train_loader = get_dataloader(
            batch_size=4, 
            mode='train', 
            max_samples_per_class=100,
            num_workers=0  # Set to 0 for Windows compatibility
        )
        
        # Get one batch
        images, physics, labels = next(iter(train_loader))
        
        print(f"  Batch shapes:")
        print(f"  Images: {images.shape} (expected: [4, 2, 80, 48] or [4, 2, 64, 64] for legacy)")
        print(f"  Physics: {physics.shape} (expected: [4, 4])")
        print(f"  Labels: {labels.shape} (expected: [4])")
        print(f"  Label values: {labels.tolist()} (should be 0 or 1)")
        print(f"  DataLoader test passed!")
        
    except FileNotFoundError as e:
        print(f"  Warning: {e}")
        print("  This is expected if running outside the project directory or without data files.")
    except Exception as e:
        print(f"  Error: {e}")
        raise
