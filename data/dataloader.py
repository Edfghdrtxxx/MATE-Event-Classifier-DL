import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import h5py
from pathlib import Path
from typing import Tuple, Optional, List
import warnings

class MATESimDataset(Dataset):
    """
    Real HDF5 dataset loader for MATE simulation data.
    
    Loads data from HDF5 files generated by MATESIM toolkit for binary classification
    of 3He vs 4He particles in the MATE-TPC experiment.
    
    Data structure:
        - images: (N, 64, 64, 2) - TPC charge deposition and drift time projections
        - physics_features: (N, 4) - Moment of Inertia tensor features (I_xx, I_yy, I_xy, Eigen_Ratio)
        - labels: (N,) - Particle type labels (0: 3He, 1: 4He)
    """
    def __init__(self, 
                 data_dir: str = "../dataset/HDF5_Form",
                 mode: str = 'train',
                 train_split: float = 0.8,
                 max_samples_per_class: Optional[int] = None,
                 normalize: bool = True,
                 seed: int = 42):
        """
        Initialize MATE simulation dataset.
        
        Args:
            data_dir: Directory containing HDF5 files
            mode: 'train', 'val', or 'test'
            train_split: Fraction of data to use for training (rest split equally for val/test)
            max_samples_per_class: Maximum samples to load per class (None = load all)
            normalize: Whether to normalize images to [0, 1]
            seed: Random seed for reproducible train/val/test split
        """
        self.data_dir = Path(data_dir)
        self.mode = mode
        self.normalize = normalize
        
        # Define file paths for binary classification: 3He (label 0) vs 4He (label 1)
        self.file_mapping = {
            0: "sim_inv_12C300MeV_4He_3He_25k.h5",  # 3He particles
            1: "sim_inv_12C300MeV_4He_4He_25k.h5"   # 4He (alpha) particles
        }
        
        # Load data from HDF5 files
        self.images, self.physics_features, self.labels = self._load_data(max_samples_per_class)
        
        # Split data into train/val/test
        self._split_data(train_split, seed)
        
        print(f"✓ Loaded {len(self)} samples for {mode} mode")
        
    def _load_data(self, max_samples: Optional[int]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Load data from HDF5 files."""
        all_images = []
        all_physics = []
        all_labels = []
        
        for label, filename in self.file_mapping.items():
            filepath = self.data_dir / filename
            
            if not filepath.exists():
                raise FileNotFoundError(
                    f"Data file not found: {filepath}\n"
                    f"Please ensure MATE simulation data is available in {self.data_dir}"
                )
            
            with h5py.File(filepath, 'r') as f:
                # Load images (N, 64, 64, 2)
                images = f['images'][:]
                
                # Load physics features (N, 4)
                physics = f['physics_features'][:]
                
                # Limit samples if specified
                if max_samples is not None:
                    images = images[:max_samples]
                    physics = physics[:max_samples]
                
                n_samples = len(images)
                
                all_images.append(images)
                all_physics.append(physics)
                all_labels.append(np.full(n_samples, label, dtype=np.int64))
                
                print(f"  Loaded {n_samples} samples from {filename} (label={label})")
        
        # Concatenate all data
        images = np.concatenate(all_images, axis=0)
        physics = np.concatenate(all_physics, axis=0)
        labels = np.concatenate(all_labels, axis=0)
        
        return images, physics, labels
    
    def _split_data(self, train_split: float, seed: int):
        """Split data into train/val/test sets."""
        np.random.seed(seed)
        n_total = len(self.labels)
        indices = np.random.permutation(n_total)
        
        # Calculate split points
        n_train = int(n_total * train_split)
        n_val = int(n_total * (1 - train_split) / 2)
        
        if self.mode == 'train':
            indices = indices[:n_train]
        elif self.mode == 'val':
            indices = indices[n_train:n_train + n_val]
        elif self.mode == 'test':
            indices = indices[n_train + n_val:]
        else:
            raise ValueError(f"Invalid mode: {self.mode}. Must be 'train', 'val', or 'test'")
        
        self.images = self.images[indices]
        self.physics_features = self.physics_features[indices]
        self.labels = self.labels[indices]
    
    def __len__(self) -> int:
        return len(self.labels)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Get a single sample.
        
        Returns:
            Tuple of (image, physics_features, label)
                - image: (2, 64, 64) tensor
                - physics_features: (4,) tensor
                - label: scalar tensor
        """
        # Load image (64, 64, 2) and convert to (2, 64, 64)
        image = self.images[idx].transpose(2, 0, 1).astype(np.float32)
        
        # Normalize to [0, 1] if requested
        if self.normalize:
            # Normalize each channel independently
            for c in range(image.shape[0]):
                channel = image[c]
                min_val, max_val = channel.min(), channel.max()
                if max_val > min_val:
                    image[c] = (channel - min_val) / (max_val - min_val)
        
        # Load physics features
        physics = self.physics_features[idx].astype(np.float32)
        
        # Load label
        label = self.labels[idx]
        
        return torch.from_numpy(image), torch.from_numpy(physics), torch.tensor(label, dtype=torch.long)


def get_dataloader(batch_size: int = 32, 
                   mode: str = 'train',
                   data_dir: str = "../dataset/HDF5_Form",
                   num_workers: int = 4,
                   max_samples_per_class: Optional[int] = None) -> DataLoader:
    """
    Factory function for MATE DataLoaders.
    
    Args:
        batch_size: Batch size for training
        mode: 'train', 'val', or 'test'
        data_dir: Directory containing HDF5 files
        num_workers: Number of worker processes for data loading
        max_samples_per_class: Maximum samples per class (for quick testing)
    
    Returns:
        DataLoader instance
    """
    dataset = MATESimDataset(
        data_dir=data_dir,
        mode=mode,
        max_samples_per_class=max_samples_per_class
    )
    
    return DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=(mode == 'train'),
        num_workers=num_workers,
        pin_memory=True
    )


if __name__ == "__main__":
    # Test the dataloader
    print("=== Testing MATE DataLoader ===")
    
    try:
        # Test with small sample for quick verification
        train_loader = get_dataloader(batch_size=4, mode='train', max_samples_per_class=100)
        
        # Get one batch
        images, physics, labels = next(iter(train_loader))
        
        print(f"✓ Batch shapes:")
        print(f"  Images: {images.shape} (expected: [4, 2, 64, 64])")
        print(f"  Physics: {physics.shape} (expected: [4, 4])")
        print(f"  Labels: {labels.shape} (expected: [4])")
        print(f"  Label values: {labels.tolist()} (should be 0 or 1)")
        print(f"✓ DataLoader test passed!")
        
    except FileNotFoundError as e:
        print(f"⚠ Warning: {e}")
        print("  This is expected if running outside the project directory.")
    except Exception as e:
        print(f"✗ Error: {e}")
        raise
